{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3DWSFq+KlMMzzyPFA2rB6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshalDharpure/APR-Assignment-_1-LR/blob/main/Logistic_regression_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ooeKHoWG_757"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf6bd8af"
      },
      "source": [
        "# Task\n",
        "Analyze the \"Movie_collection_train.csv\" dataset using Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e89a77bb"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "### Subtask:\n",
        "Load the `Movie_collection_train.csv` file into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "681f3016"
      },
      "source": [
        "**Reasoning**:\n",
        "Import pandas and load the dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ff539b6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/Movie_collection_train.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "146f7b96"
      },
      "source": [
        "## Explore the data\n",
        "\n",
        "### Subtask:\n",
        "Perform initial data exploration to understand the structure, content, and basic statistics of the dataset. This may include checking for missing values, examining data types, and looking at the distribution of key variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83399032"
      },
      "source": [
        "**Reasoning**:\n",
        "Perform initial data exploration by displaying the shape, data types, descriptive statistics, missing values, and unique values of the DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1366ae73"
      },
      "source": [
        "print(\"Shape of the DataFrame:\")\n",
        "print(df.shape)\n",
        "\n",
        "print(\"\\nData types of each column:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\nDescriptive statistics for numerical columns:\")\n",
        "display(df.describe())\n",
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nUnique values and counts for categorical columns:\")\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    print(f\"\\nColumn: {col}\")\n",
        "    print(df[col].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31e4869c"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "### Subtask:\n",
        "Prepare the data for the logistic regression model. This will involve handling missing values, encoding categorical features, and potentially scaling numerical features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f36d9ed"
      },
      "source": [
        "**Reasoning**:\n",
        "Address missing values, encode categorical features, create the target variable, and scale numerical features to prepare the data for logistic regression.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfab5e2f"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# 1. Address missing values in `Time_taken`\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "df['Time_taken'] = imputer.fit_transform(df[['Time_taken']])\n",
        "\n",
        "# 2. and 3. Identify and encode categorical columns\n",
        "# Drop MPAA_film_rating as it has only one unique value\n",
        "df = df.drop('MPAA_film_rating', axis=1)\n",
        "\n",
        "# One-hot encode 'Genre'\n",
        "df = pd.get_dummies(df, columns=['Genre'], drop_first=True)\n",
        "\n",
        "# 4. Encode binary categorical column '3D_available'\n",
        "label_encoder = LabelEncoder()\n",
        "df['3D_available'] = label_encoder.fit_transform(df['3D_available'])\n",
        "\n",
        "# 6. Create a binary target variable 'Collection_Success'\n",
        "median_collection = df['Collection'].median()\n",
        "df['Collection_Success'] = (df['Collection'] > median_collection).astype(int)\n",
        "\n",
        "# Drop the original 'Collection' column\n",
        "df = df.drop('Collection', axis=1)\n",
        "\n",
        "# 5. Scale numerical features\n",
        "# Identify numerical columns (excluding the target variable and encoded/handled categoricals)\n",
        "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "numerical_cols.remove('Collection_Success')\n",
        "# Exclude one-hot encoded genre columns and the encoded 3D_available from scaling\n",
        "cols_to_exclude_from_scaling = ['3D_available'] + [col for col in numerical_cols if col.startswith('Genre_')]\n",
        "numerical_cols_to_scale = [col for col in numerical_cols if col not in cols_to_exclude_from_scaling]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[numerical_cols_to_scale] = scaler.fit_transform(df[numerical_cols_to_scale])\n",
        "\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1f5a460"
      },
      "source": [
        "## Split the data\n",
        "\n",
        "### Subtask:\n",
        "Split the dataset into training and testing sets to evaluate the model's performance on unseen data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0a2284c"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the preprocessed data into training and testing sets for model evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7370b75d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop('Collection_Success', axis=1)\n",
        "y = df['Collection_Success']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9db74421"
      },
      "source": [
        "## Build and train the logistic regression model\n",
        "\n",
        "### Subtask:\n",
        "Initialize and train a Logistic Regression model on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e2a18d2"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize and train a Logistic Regression model on the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71d395e4"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0c6a4c1"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model's performance on the testing data using appropriate metrics (e.g., accuracy, precision, recall, F1-score).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0df057f"
      },
      "source": [
        "**Reasoning**:\n",
        "Use the trained model to make predictions on the test set and calculate the evaluation metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61dd9f10"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45cd1d4"
      },
      "source": [
        "## Interpret the results\n",
        "\n",
        "### Subtask:\n",
        "Analyze the model's coefficients to understand the relationships between the features and the target variable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21f0925d"
      },
      "source": [
        "**Reasoning**:\n",
        "Access and analyze the coefficients of the trained logistic regression model to understand feature importance and direction of influence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50f6a7da"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Access the coefficients of the trained logistic regression model\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# 2. Create a pandas Series associating coefficients with feature names\n",
        "feature_names = X_train.columns\n",
        "coef_series = pd.Series(coefficients, index=feature_names)\n",
        "\n",
        "# 3. Sort the coefficients by their absolute values\n",
        "sorted_coef = coef_series.abs().sort_values(ascending=False)\n",
        "sorted_coef_with_sign = coef_series[sorted_coef.index]\n",
        "\n",
        "# 4. Print the sorted coefficients\n",
        "print(\"Sorted Coefficients (by absolute value):\")\n",
        "display(sorted_coef_with_sign)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "924a0bae"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The dataset contains 400 rows and 19 columns, including numerical and categorical data types.\n",
        "*   The `Time_taken` column had 8 missing values, which were imputed using the mean.\n",
        "*   The `MPAA_film_rating` column was dropped as it contained only one unique value.\n",
        "*   The `Genre` column was one-hot encoded, while the `3D_available` column was label encoded.\n",
        "*   A binary target variable `Collection_Success` was created, indicating whether the movie's collection was above the median.\n",
        "*   Numerical features were scaled using `StandardScaler`.\n",
        "*   The dataset was split into training (320 samples) and testing (80 samples) sets.\n",
        "*   The trained Logistic Regression model achieved the following performance metrics on the test set:\n",
        "    *   Accuracy: 0.8500\n",
        "    *   Precision: 0.8684\n",
        "    *   Recall: 0.8250\n",
        "    *   F1-score: 0.8462\n",
        "*   Features with the largest absolute coefficients, and thus the most significant influence on `Collection_Success`, include `Trailer_views`, `Budget`, `3D_available`, and `Movie_length`. The signs of the coefficients indicate the direction of the relationship (positive or negative) with the likelihood of successful collection.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The model shows good predictive capability (85% accuracy) for classifying movie collection success based on the provided features.\n",
        "*   Further investigation into the features with the highest coefficients could provide deeper insights into what drives movie collection success.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e02cab31"
      },
      "source": [
        "# Task\n",
        "Analyze the dataset \"Movie_collection_train.csv\" using logistic regression, including data visualization, linear discriminant analysis, performance metrics, and a confusion matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63319678"
      },
      "source": [
        "## Visualize the data\n",
        "\n",
        "### Subtask:\n",
        "Create visualizations to explore relationships between features and the target variable, and to understand data distributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63218f52"
      },
      "source": [
        "**Reasoning**:\n",
        "Create visualizations to explore the relationships between features and the target variable, and to understand data distributions based on the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f3584c1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Create a histogram of the original 'Collection' column\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['Collection_Success'], kde=True) # Use Collection_Success instead of Collection\n",
        "plt.title('Distribution of Collection Success')\n",
        "plt.xlabel('Collection Success (1: Above Median, 0: Below Median)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# 2. Create a box plot of 'Collection' by '3D_available'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='3D_available', y='Collection_Success', data=df) # Use Collection_Success instead of Collection\n",
        "plt.title('Collection Success by 3D Availability')\n",
        "plt.xlabel('3D Available (1: Yes, 0: No)')\n",
        "plt.ylabel('Collection Success')\n",
        "plt.xticks([0, 1], ['No', 'Yes'])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 3. Create scatter plots for numerical features against 'Collection_Success'\n",
        "numerical_features = ['Marketin_expense', 'Production_expense', 'Budget', 'Trailer_views']\n",
        "for feature in numerical_features:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(x=feature, y='Collection_Success', data=df)\n",
        "    plt.title(f'Collection Success vs {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Collection Success')\n",
        "    plt.show()\n",
        "\n",
        "# 4. Create bar plots for categorical features ('Genre' was one-hot encoded, 'MPAA_film_rating' was dropped)\n",
        "# We can visualize the distribution of the one-hot encoded Genre columns\n",
        "genre_cols = [col for col in df.columns if col.startswith('Genre_')]\n",
        "for col in genre_cols:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    df[col].value_counts().plot(kind='bar')\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "# Visualize the distribution of '3D_available' (which is now numerical)\n",
        "plt.figure(figsize=(8, 5))\n",
        "df['3D_available'].value_counts().plot(kind='bar')\n",
        "plt.title('Distribution of 3D Availability')\n",
        "plt.xlabel('3D Available (1: Yes, 0: No)')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks([0, 1], ['No', 'Yes'], rotation=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c144cc8a"
      },
      "source": [
        "## Perform linear discriminant analysis (lda)\n",
        "\n",
        "### Subtask:\n",
        "Apply LDA for dimensionality reduction or classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19a57cce"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply LDA for dimensionality reduction and transform the training and testing data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9b56e57"
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Instantiate LinearDiscriminantAnalysis\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "\n",
        "# Fit LDA model to the training data and transform\n",
        "X_train_lda = lda.fit_transform(X_train, y_train)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test_lda = lda.transform(X_test)\n",
        "\n",
        "print(\"Shape of X_train_lda:\", X_train_lda.shape)\n",
        "print(\"Shape of X_test_lda:\", X_test_lda.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4e39bf3"
      },
      "source": [
        "## Build and train the logistic regression model\n",
        "\n",
        "### Subtask:\n",
        "Initialize and train a Logistic Regression model on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aabe546f"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize and train a Logistic Regression model on the LDA-transformed training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43d1da2a"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model_lda = LogisticRegression()\n",
        "model_lda.fit(X_train_lda, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa6b7c57"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model's performance on the testing data using appropriate metrics (e.g., accuracy, precision, recall, F1-score), including a confusion matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "306a5a6c"
      },
      "source": [
        "**Reasoning**:\n",
        "Use the trained LDA-based logistic regression model to make predictions on the transformed test set and calculate the evaluation metrics and the confusion matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d74fa67d"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lda = model_lda.predict(X_test_lda)\n",
        "\n",
        "# Calculate and print the evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_lda)\n",
        "precision = precision_score(y_test, y_pred_lda)\n",
        "recall = recall_score(y_test, y_pred_lda)\n",
        "f1 = f1_score(y_test, y_pred_lda)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "# Calculate and print the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_lda)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebf0eac8"
      },
      "source": [
        "## Interpret the results\n",
        "\n",
        "### Subtask:\n",
        "Analyze the model's coefficients to understand the relationships between the features and the target variable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ceeff94"
      },
      "source": [
        "**Reasoning**:\n",
        "Access and print the coefficient of the trained logistic regression model which was trained on LDA-transformed data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2f69c64"
      },
      "source": [
        "# Access the coefficient of the trained logistic regression model\n",
        "coefficient_lda = model_lda.coef_[0]\n",
        "\n",
        "# Print the coefficient\n",
        "print(\"Coefficient of the Logistic Regression model after LDA:\")\n",
        "print(coefficient_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9abfc32e"
      },
      "source": [
        "## Summarize observations and insights\n",
        "\n",
        "### Subtask:\n",
        "Summarize the key observations from the data exploration, visualization, model performance, and feature interpretation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd17db58"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The distribution of `Collection_Success` is roughly balanced between movies that collected above and below the median.\n",
        "*   Visualizations suggest that while higher `Marketin_expense`, `Production_expense`, `Budget`, and `Trailer_views` are associated with a higher probability of collection success, there is significant overlap between the two classes.\n",
        "*   Linear Discriminant Analysis (LDA) successfully reduced the dimensionality of the data to 1 for the binary classification task.\n",
        "*   The logistic regression model trained on the LDA-transformed data achieved an accuracy of 0.8625 on the test set.\n",
        "*   The model demonstrated a precision of 0.8718, a recall of 0.8500, and an F1-score of 0.8608.\n",
        "*   The confusion matrix shows that the model correctly predicted 34 true positives and 35 true negatives, while making 5 false positive and 6 false negative predictions.\n",
        "*   The logistic regression model trained on the single LDA component has a coefficient of approximately 2.04, indicating a positive relationship between the LDA component (which represents a combination of original features optimized for class separation) and the likelihood of collection success.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Investigate the specific combination of original features that the single LDA component represents to gain a deeper understanding of which factors contribute most significantly to collection success according to the model.\n",
        "*   Explore alternative dimensionality reduction techniques or feature selection methods to see if they yield improved model performance or more interpretable results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e78a240b"
      },
      "source": [
        "# Movie Collection Analysis Report\n",
        "\n",
        "## 1. Load the dataset\n",
        "\n",
        "**Code:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80b1fcba"
      },
      "source": [
        "print(\"Shape of the DataFrame:\")\n",
        "print(df.shape)\n",
        "\n",
        "print(\"\\nData types of each column:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\nDescriptive statistics for numerical columns:\")\n",
        "display(df.describe())\n",
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nUnique values and counts for categorical columns:\")\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    print(f\"\\nColumn: {col}\")\n",
        "    print(df[col].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d3fac81"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Create a histogram of the original 'Collection' column\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['Collection_Success'], kde=True)\n",
        "plt.title('Distribution of Collection Success')\n",
        "plt.xlabel('Collection Success (1: Above Median, 0: Below Median)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# 2. Create a box plot of 'Collection' by '3D_available'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='3D_available', y='Collection_Success', data=df)\n",
        "plt.title('Collection Success by 3D Availability')\n",
        "plt.xlabel('3D Available (1: Yes, 0: No)')\n",
        "plt.ylabel('Collection Success')\n",
        "plt.xticks([0, 1], ['No', 'Yes'])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 3. Create scatter plots for numerical features against 'Collection_Success'\n",
        "numerical_features = ['Marketin_expense', 'Production_expense', 'Budget', 'Trailer_views']\n",
        "for feature in numerical_features:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(x=feature, y='Collection_Success', data=df)\n",
        "    plt.title(f'Collection Success vs {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Collection Success')\n",
        "    plt.show()\n",
        "\n",
        "# 4. Create bar plots for categorical features ('Genre' was one-hot encoded, 'MPAA_film_rating' was dropped)\n",
        "# We can visualize the distribution of the one-hot encoded Genre columns\n",
        "genre_cols = [col for col in df.columns if col.startswith('Genre_')]\n",
        "for col in genre_cols:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    df[col].value_counts().plot(kind='bar')\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "# Visualize the distribution of '3D_available' (which is now numerical)\n",
        "plt.figure(figsize=(8, 5))\n",
        "df['3D_available'].value_counts().plot(kind='bar')\n",
        "plt.title('Distribution of 3D Availability')\n",
        "plt.xlabel('3D Available (1: Yes, 0: No)')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks([0, 1], ['No', 'Yes'], rotation=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2363513c"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# 1. Address missing values in `Time_taken`\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "df['Time_taken'] = imputer.fit_transform(df[['Time_taken']])\n",
        "\n",
        "# 2. and 3. Identify and encode categorical columns\n",
        "# Drop MPAA_film_rating as it has only one unique value\n",
        "# df = df.drop('MPAA_film_rating', axis=1) # This line caused the error\n",
        "\n",
        "# One-hot encode 'Genre'\n",
        "# df = pd.get_dummies(df, columns=['Genre'], drop_first=True) # This line caused the error\n",
        "\n",
        "# 4. Encode binary categorical column '3D_available'\n",
        "label_encoder = LabelEncoder()\n",
        "df['3D_available'] = label_encoder.fit_transform(df['3D_available'])\n",
        "\n",
        "# 6. Create a binary target variable 'Collection_Success'\n",
        "# median_collection = df['Collection'].median() # This line caused the error\n",
        "# df['Collection_Success'] = (df['Collection'] > median_collection).astype(int) # This line also caused the error\n",
        "\n",
        "# Drop the original 'Collection' column\n",
        "# df = df.drop('Collection', axis=1) # This line caused the error\n",
        "\n",
        "# 5. Scale numerical features\n",
        "# Identify numerical columns (excluding the target variable and encoded/handled categoricals)\n",
        "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "numerical_cols.remove('Collection_Success')\n",
        "# Exclude one-hot encoded genre columns and the encoded 3D_available from scaling\n",
        "cols_to_exclude_from_scaling = ['3D_available'] + [col for col in numerical_cols if col.startswith('Genre_')]\n",
        "numerical_cols_to_scale = [col for col in numerical_cols if col not in cols_to_exclude_from_scaling]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[numerical_cols_to_scale] = scaler.fit_transform(df[numerical_cols_to_scale])\n",
        "\n",
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f1bb0c4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop('Collection_Success', axis=1)\n",
        "y = df['Collection_Success']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b3ed9c8"
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Instantiate LinearDiscriminantAnalysis\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "\n",
        "# Fit LDA model to the training data and transform\n",
        "X_train_lda = lda.fit_transform(X_train, y_train)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test_lda = lda.transform(X_test)\n",
        "\n",
        "print(\"Shape of X_train_lda:\", X_train_lda.shape)\n",
        "print(\"Shape of X_test_lda:\", X_test_lda.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0a3806b"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model_lda = LogisticRegression()\n",
        "model_lda.fit(X_train_lda, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbba6582"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_lda = model_lda.predict(X_test_lda)\n",
        "\n",
        "# Calculate and print the evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_lda)\n",
        "precision = precision_score(y_test, y_pred_lda)\n",
        "recall = recall_score(y_test, y_pred_lda)\n",
        "f1 = f1_score(y_test, y_pred_lda)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "# Calculate and print the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_lda)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95a6d4dd"
      },
      "source": [
        "# Access the coefficient of the trained logistic regression model\n",
        "coefficient_lda = model_lda.coef_[0]\n",
        "\n",
        "# Print the coefficient\n",
        "print(\"Coefficient of the Logistic Regression model after LDA:\")\n",
        "print(coefficient_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59df6ab3"
      },
      "source": [
        "## Summarize observations and insights\n",
        "\n",
        "### Subtask:\n",
        "Summarize the key observations from the data exploration, visualization, model performance, and feature interpretation."
      ]
    }
  ]
}